"""
Transform Silver Layer - Merge and Save Combined Dataset
H·ª£p nh·∫•t Fake v√† Real datasets, th√™m labels, v√† l∆∞u d∆∞·ªõi d·∫°ng Parquet s·ª≠ d·ª•ng Spark
"""
from dagster import asset, AssetExecutionContext, Output, MetadataValue, AssetIn
from datetime import datetime
from pyspark.sql import SparkSession
import pandas as pd
import os
from pipeline.utils.spark import create_spark_session

@asset(
    description="Merge Fake v√† Real datasets, th√™m labels v√† l∆∞u th√†nh Parquet s·ª≠ d·ª•ng Spark",
    compute_kind='Spark',
    required_resource_keys={'minio_resource'},
    group_name='silver_layer',
    ins={
        'load_Fake.csv': AssetIn('load_fake_dataset'),
        'load_Real.csv': AssetIn('load_real_dataset'),
    }
)
def combined_news_dataset(
    context: AssetExecutionContext,
    **kwargs
) -> Output[dict]: 
    minio_client = context.resources.minio_resource
    
    fake_df = kwargs.get('load_Fake.csv')
    real_df = kwargs.get('load_Real.csv')
    
    if fake_df is None or real_df is None:
        context.log.error("Kh√¥ng th·ªÉ load ƒë∆∞·ª£c datasets t·ª´ upstream assets")
        return Output(
            value={"status": "error", "message": "Missing upstream data"},
            metadata={"error": "upstream_data_missing"}
        )
    
    context.log.info(f"Loaded Fake dataset: {len(fake_df)} records")
    context.log.info(f"Loaded Real dataset: {len(real_df)} records")
    
    # Th√™m label cho m·ªói dataset
    fake_df['label'] = 0  # 0 = Fake
    real_df['label'] = 1  # 1 = Real
    
    # Merge hai datasets
    combined_df = pd.concat([fake_df, real_df], ignore_index=True)
    
    # Shuffle data
    combined_df = combined_df.sample(frac=1, random_state=42).reset_index(drop=True)
    
    total_records = len(combined_df)
    fake_count = (combined_df['label'] == 0).sum()
    real_count = (combined_df['label'] == 1).sum()
    
    context.log.info(f"üìä Combined dataset: {total_records} records")
    context.log.info(f"   - Fake: {fake_count} ({fake_count/total_records*100:.1f}%)")
    context.log.info(f"   - Real: {real_count} ({real_count/total_records*100:.1f}%)")
    
    # T·∫°o SparkSession ƒë∆°n gi·∫£n
    spark = create_spark_session ('Union file')
    
    context.log.info("‚úÖ SparkSession created")
    
    # Convert Pandas DataFrame to Spark DataFrame
    spark_df = spark.createDataFrame(combined_df)
    
    # ƒê·∫£m b·∫£o silver bucket t·ªìn t·∫°i
    silver_bucket = 'silver'
    if not minio_client.bucket_exists(silver_bucket):
        context.log.info(f"ü™£ T·∫°o silver bucket: {silver_bucket}")
        minio_client.make_bucket(silver_bucket)
    
    # L∆∞u file Parquet v√†o MinIO s·ª≠ d·ª•ng Spark
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_path = f"s3a://{silver_bucket}/combined_news_{timestamp}.parquet"
    
    try:
        context.log.info(f"üíæ ƒêang l∆∞u Parquet file v·ªõi Spark: {output_path}")
        
        # Write to S3/MinIO using Spark
        spark_df.coalesce(1).write \
            .mode("overwrite") \
            .parquet(output_path)
        
        context.log.info(f"‚úÖ ƒê√£ l∆∞u th√†nh c√¥ng: {output_path}")
        
        # Get file size (approximate from MinIO)
        objects = list(minio_client.list_objects(silver_bucket, recursive=True))
        parquet_files = [obj for obj in objects if 'combined_news' in obj.object_name and obj.object_name.endswith('.parquet')]
        total_size = sum(obj.size for obj in parquet_files)
        
        result = {
            "status": "success",
            "output_path": output_path,
            "total_records": total_records,
            "fake_count": int(fake_count),
            "real_count": int(real_count),
            "file_size_bytes": total_size,
        }
        
        # Stop Spark session
        spark.stop()
        context.log.info("üõë SparkSession stopped")
        
        return Output(
            value=result,
            metadata={
                "total_records": total_records,
                "fake_count": int(fake_count),
                "real_count": int(real_count),
                "fake_percentage": MetadataValue.float(fake_count/total_records*100),
                "real_percentage": MetadataValue.float(real_count/total_records*100),
                "output_path": output_path,
                "file_size_bytes": total_size,
                "columns": MetadataValue.text(", ".join(combined_df.columns)),
                "distribution": MetadataValue.md(f"""
**Dataset Distribution:**
- Total Records: {total_records:,}
- Fake News: {fake_count:,} ({fake_count/total_records*100:.1f}%)
- Real News: {real_count:,} ({real_count/total_records*100:.1f}%)

**Output:**
- Format: Parquet
- Engine: Apache Spark
- Location: {output_path}
- Size: {total_size:,} bytes
                """),
            }
        )
        
    except Exception as e:
        context.log.error(f"‚ùå L·ªói khi l∆∞u file: {str(e)}")
        spark.stop()
        raise
